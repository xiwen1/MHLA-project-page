<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head - Kewei Zhang, Ye Huang, et, al.">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution and few self-attention blocks) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6% improvement on ImageNet classification, a 12.6% gain in image generation tasks with the same computational complexity and a 2.1% improvement on NLP.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="Linear Attention, Model Architecture, Efficiency">
  <!-- TODO: List all authors -->
  <meta name="author" content="Kewei Zhang, Ye Huang, Yufan Deng, Jincheng YU, Junsong Chen, Huan Ling, Enze Xie, Daquan Zhou ">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="Peking University, Magic Group">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://xiwen1.github.io/MHLA-project-page/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="Kewei Zhang">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head">
  <meta name="citation_author" content="FIRST_AUTHOR_LAST, FIRST_AUTHOR_FIRST">
  <meta name="citation_author" content="SECOND_AUTHOR_LAST, SECOND_AUTHOR_FIRST">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "Kewei Zhang",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://xiwen1.github.io/MHLA-project-page/",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["Linear Attention", "Model Architecture", "Efficiency"],
    "abstract": "While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution and few self-attention blocks) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6% improvement on ImageNet classification, a 12.6% gain in image generation tasks with the same computational complexity and a 2.1% improvement on NLP.",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://xiwen1.github.io/MHLA-project-page/"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  
  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">
              <span class="gradient-text">MHLA</span>:
              Restoring Expressivity of 
              Linear Attention via <br> <span class="accent-text">Token-Level Multi-Head</span>
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper Authors -->
              <span class="author-block">
                <a href="https://xiwen1.github.io/">Kewei Zhang</a><sup>1*</sup>,</span>
              <span class="author-block">
                <a href="https://yeyeah.life/">Ye Huang</a><sup>1*</sup>,</span>
              <span class="author-block">
                <a href="#">Yufan Deng</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="#">Jincheng Yu</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://lawrence-cj.github.io/">Junsong Chen</a><sup>2</sup>,</span> <br>
              <span class="author-block">
                <a href="https://www.cs.toronto.edu/~linghuan/">Huan Ling</a><sup>2</sup>,</span> 
              <span class="author-block">
                <a href="https://xieenze.github.io/">Enze Xie</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://zhoudaquan.github.io/homepage.io/">Daquan Zhou</a><sup>1</sup></span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block"><sup>1</sup>Peking University</span> 
                    <span class="author-block"><sup>2</sup>NVIDIA</span>
                    <!-- TODO: Remove this line if no equal contribution -->
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- TODO: Add your supplementary material PDF or remove this section
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- MHLA Pipeline -->
<section class="section pipeline-section">
  <div class="container is-max-desktop">
    <div class="pipeline-card">
      <img src="static/images/MHLA_pipeline.png" alt="MHLA pipeline overview" class="pipeline-image" loading="lazy" />
      <p class="pipeline-caption">Figure: MHLA pipeline overview.</p>
    </div>
  </div>
</section>
<!-- End MHLA Pipeline -->

<!-- MHLA Performance Highlights -->
<section class="section performance-section">
  <div class="container is-max-desktop">
    <div class="performance-header">
      <h2 class="performance-title">A <span class="title-highlight">Universal</span> High-Efficiency Linear Attention Operator</h2>
    </div>
    
    <div class="performance-grid">
      <!-- Image Classification -->
      <div class="performance-card">
        <h3 class="performance-card-title">Image Classification</h3>
        <div class="performance-metrics">
          <div class="metric-item">
            <span class="metric-label">Performance</span>
            <span class="metric-value">+3.6%</span>
          </div>
          <div class="metric-separator"></div>
          <div class="metric-item">
            <span class="metric-label">Complexity</span>
            <span class="metric-badge">Linear</span>
          </div>
        </div>
      </div>

      <!-- Image Generation -->
      <div class="performance-card">
        <h3 class="performance-card-title">Image Generation</h3>
        <div class="performance-metrics">
          <div class="metric-item">
            <span class="metric-label">Performance</span>
            <span class="metric-value">+12.6%</span>
          </div>
          <div class="metric-separator"></div>
          <div class="metric-item">
            <span class="metric-label">Complexity</span>
            <span class="metric-badge">Linear</span>
          </div>
        </div>
      </div>

      <!-- Language Modeling -->
      <div class="performance-card">
        <h3 class="performance-card-title">Language Modeling</h3>
        <div class="performance-metrics">
          <div class="metric-item">
            <span class="metric-label">Performance</span>
            <span class="metric-value">+6.3%</span>
          </div>
          <div class="metric-separator"></div>
          <div class="metric-item">
            <span class="metric-label">Complexity</span>
            <span class="metric-badge">Linear</span>
          </div>
        </div>
      </div>

      <!-- Video Generation -->
      <div class="performance-card">
        <h3 class="performance-card-title">Video Generation</h3>
        <div class="performance-metrics">
          <div class="metric-item">
            <span class="metric-label">Performance</span>
            <span class="metric-value">+41%</span>
          </div>
          <div class="metric-separator"></div>
          <div class="metric-item">
            <span class="metric-label">Complexity</span>
            <span class="metric-badge">Linear</span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End MHLA Performance Highlights -->



<!-- MHLA Key Features -->
<section class="section features-section">
  <div class="container is-max-desktop">
    <div class="features-header">
      <h2 class="features-title">Why MHLA?</h2>
      <p class="features-subtitle">Key Features that Set MHLA Apart</p>
    </div>

    <div class="features-grid">
      <!-- Feature 1 -->
      <div class="feature-card">
        <div class="feature-icon">
          <i class="fas fa-bolt"></i>
        </div>
        <h3 class="feature-title">Optimal Efficiency</h3>
        <p class="feature-description">
          For sequence lengths >1k, <br><span class="feature-highlight is-size-5">MHLA surpasses Flash Attention</span> in speed. Maintains identical complexity with vanilla linear attention with <span class="feature-highlight">zero overhead</span>。
        </p>
      </div>

      <!-- Feature 2 -->
      <div class="feature-card">
        <div class="feature-icon">
          <i class="fas fa-arrows-alt-h"></i>
        </div>
        <h3 class="feature-title">Flexible Attention Forms</h3>
        <p class="feature-description">
          Natively supports both <span class="feature-highlight is-size-5">causal</span> and <span class="feature-highlight is-size-5">non-causal</span> attention modes with built-in <span class="feature-highlight">chunkwise training</span> compatibility for enhanced flexibility.
        </p>
      </div>

      <!-- Feature 3 -->
      <div class="feature-card">
        <div class="feature-icon">
          <i class="fas fa-star"></i>
        </div>
        <h3 class="feature-title">Token-Level Diversity</h3>
        <p class="feature-description">
          Introduces diversity at the token level to <span class="feature-highlight is-size-5">break global context collapse</span> in linear attention, unlocking significant performance gains.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End MHLA Key Features -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">About MHLA</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution and few self-attention blocks) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a <span class="metric-highlight">3.6%</span> improvement on ImageNet classification, a <span class="metric-highlight">6.3%</span> gain on NLP, a <span class="metric-highlight">12.6%</span> improvement in image generation tasks and a <span class="metric-highlight">41%</span> enhancement in video generation tasks with the same computational complexity.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Compact Case Gallery -->
<section class="section case-gallery-section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 performance-title has-text-centered">Image Generation</h2>
    
    <!-- MHLA vs Flash Attention - Speed & FID Chart -->
    <section class="chart-section" aria-label="MHLA vs Flash Attention speed and FID comparison">
      <div class="chart-card">
        <h3 class="title is-5 has-text-centered">Speed Comparison (Throughput)</h3>
        <p class="has-text-centered normal-highlight is-size-6"><span class="title-highlight">8.2× speed up</span> compared with Flash Attention</p>
        <div class="bar-chart" role="img" aria-label="Bar chart showing MHLA throughput 7.40 and Flash Attention throughput 0.90">
          <div class="bar-row">
            <span class="bar-label">MHLA</span>
            <div class="bar-track">
              <div class="bar-fill mhla" style="--value: 7.40" aria-label="MHLA throughput 7.40"></div>
            </div>
            <span class="bar-value">7.40</span>
          </div>
          <div class="bar-row">
            <span class="bar-label">Flash Attn</span>
            <div class="bar-track">
              <div class="bar-fill flash" style="--value: 0.90" aria-label="Flash Attention throughput 0.90"></div>
            </div>
            <span class="bar-value">0.90</span>
          </div>
        </div>

        <h3 class="title is-5 has-text-centered fid-title">Quality Comparison (FID↓)</h3>
        <!-- <p class="has-text-centered is-size-6">Lower is better: MHLA 59.8 vs Flash Attention 68.4</p> -->
        <div class="bar-chart fid-chart" role="img" aria-label="Bar chart showing FID scores: MHLA 59.8 and Flash Attention 68.4 (lower is better)">
          <div class="bar-row">
            <span class="bar-label">MHLA</span>
            <div class="bar-track">
              <div class="bar-fill mhla" style="--value: 30" aria-label="MHLA FID 59.8"></div>
            </div>
            <span class="bar-value">59.8</span>
          </div>
          <div class="bar-row">
            <span class="bar-label">Flash Attn</span>
            <div class="bar-track">
              <div class="bar-fill flash" style="--value: 68.4" aria-label="Flash Attention FID 68.4"></div>
            </div>
            <span class="bar-value">68.4</span>
          </div>
        </div>
      </div>
    </section>

    <div class="gallery-container">
      <div class="gallery" aria-label="Case image gallery">
        <div class="gallery-item item1"><img src="static/images/case/0_512x512.png" alt="Case 0 square" loading="lazy"></div>
        <div class="gallery-item item2"><img src="static/images/case/215_800x400.png" alt="Case 6 wide" loading="lazy"></div>
        <div class="gallery-item item3"><img src="static/images/case/10_400x800.png" alt="Case 10 tall" loading="lazy"></div>
        <div class="gallery-item item4"><img src="static/images/case/70_800x400.png" alt="Case 70 wide" loading="lazy"></div>
        
        <div class="gallery-item item6"><img src="static/images/case/midjourney10_800x400.png" alt="Midjourney 10 wide" loading="lazy"></div>
        <div class="gallery-item item5"><img src="static/images/case/12_512x512.png" alt="Case 12 square" loading="lazy"></div>
        <div class="gallery-item item7"><img src="static/images/case/COCOval2014000000211560_400x800.png" alt="Case 122 tall" loading="lazy"></div>
        <div class="gallery-item item8"><img src="static/images/case/254_400x800.png" alt="Case 254 tall" loading="lazy"></div>
        <div class="gallery-item item9"><img src="static/images/case/120_512x512.png" alt="Case 120 square" loading="lazy"></div>
        <div class="gallery-item item10"><img src="static/images/case/COCOval2014000000441411_800x400.png" alt="COCO val wide" loading="lazy"></div>
        <div class="gallery-item item11"><img src="static/images/case/6_800x400.png" alt="COCO val tall" loading="lazy"></div>
        <!-- <div class="gallery-item item12"><img src="static/images/case/diffusiondb17_800x400.png" alt="DiffusionDB 17 wide" loading="lazy"></div> -->
        <!-- <div class="gallery-item item13"><img src="static/images/case/partiprompts262_800x400.png" alt="Parti prompts 262 wide" loading="lazy"></div>
        <div class="gallery-item item14"><img src="static/images/case/diffusiondb0_400x800.png" alt="DiffusionDB 0 tall" loading="lazy"></div>
        <div class="gallery-item item15"><img src="static/images/case/drawtext24_512x512.png" alt="DrawText 24 square" loading="lazy"></div>
        <div class="gallery-item item16"><img src="static/images/case/diffusiondb12_512x512.png" alt="DiffusionDB 12 square" loading="lazy"></div>
        <div class="gallery-item item17"><img src="static/images/case/diffusiondb20_512x512.png" alt="DiffusionDB 20 square" loading="lazy"></div>
        <div class="gallery-item item18"><img src="static/images/case/diffusiondb5_512x512.png" alt="DiffusionDB 5 square" loading="lazy"></div>
        <div class="gallery-item item19"><img src="static/images/case/diffusiondb6_512x512.png" alt="DiffusionDB 6 square" loading="lazy"></div>
        <div class="gallery-item item20"><img src="static/images/case/partiprompts243_512x512.png" alt="Parti prompts 243 square" loading="lazy"></div>
        <div class="gallery-item item21"><img src="static/images/case/partiprompts27_400x800.png" alt="Parti prompts 27 tall" loading="lazy"></div>
        <div class="gallery-item item22"><img src="static/images/case/partiprompts48_512x512.png" alt="Parti prompts 48 square" loading="lazy"></div>
        <div class="gallery-item item23"><img src="static/images/case/partiprompts50_512x512.png" alt="Parti prompts 50 square" loading="lazy"></div>
        <div class="gallery-item item24"><img src="static/images/case/COCOval2014000000425925_512x512.png" alt="COCO val square" loading="lazy"></div> -->
      </div>
    </div>
    <p class="subtitle is-6 gallery-subtitle has-text-centered">Images generated with SANA-MHLA</p>
  </div>
</section>



<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
  
      <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata">
  
        <source src="static/videos/banner_video.mp4" type="video/mp4">
      </video>

      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section>  -->
<!-- End teaser video -->


<!-- Image carousel (commented out)
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/carousel1.jpg" alt="First research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel2.jpg" alt="Second research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel3.jpg" alt="Third research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      <img src="static/images/carousel4.jpg" alt="Fourth research result visualization" loading="lazy"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
-->
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">

      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">

            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video1" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/carousel1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video2" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/carousel2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <!-- TODO: Add poster image for better preview -->
          <video poster="" id="video3" controls muted loop height="100%" preload="metadata">
            <!-- Your video file here -->
            <source src="static/videos/carousel3.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->






<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <!-- TODO: Replace with your poster PDF -->
      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{MHLA2025,
  title={MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head},
  author={Zhang, Kewei and Huang, Ye and Deng, Yufan and Yu, Jincheng and Chen, Junsong and Ling, Huan and Xie, Enze and Zhou, Daquan},
  journal={Conference/Journal Name},
  year={2025},
  url={https://your-domain.com/your-project-page}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            <!-- This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page. -->
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
