<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head - Kewei Zhang, Ye Huang, et, al.">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution and few self-attention blocks) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6% improvement on ImageNet classification, a 12.6% gain in image generation tasks with the same computational complexity and a 2.1% improvement on NLP.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="Linear Attention, Model Architecture, Efficiency">
  <!-- TODO: List all authors -->
  <meta name="author" content="Kewei Zhang, Ye Huang, Yufan Deng, Jincheng YU, Junsong Chen, Huan Ling, Enze Xie, Daquan Zhou ">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="Peking University, Magic Group">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://xiwen1.github.io/MHLA-project-page/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="Kewei Zhang">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head">
  <meta name="citation_author" content="FIRST_AUTHOR_LAST, FIRST_AUTHOR_FIRST">
  <meta name="citation_author" content="SECOND_AUTHOR_LAST, SECOND_AUTHOR_FIRST">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "Kewei Zhang",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://xiwen1.github.io/MHLA-project-page/",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["Linear Attention", "Model Architecture", "Efficiency"],
    "abstract": "While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution and few self-attention blocks) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6% improvement on ImageNet classification, a 12.6% gain in image generation tasks with the same computational complexity and a 2.1% improvement on NLP.",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://xiwen1.github.io/MHLA-project-page/"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  
  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">
              <span class="gradient-text">MHLA</span>:
              Restoring Expressivity of 
              Linear Attention via <br> <span class="accent-text">Token-Level Multi-Head</span>
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper Authors -->
              <span class="author-block">
                <a href="https://xiwen1.github.io/">Kewei Zhang</a><sup>1*</sup>,</span>
              <span class="author-block">
                <a href="https://yeyeah.life/">Ye Huang</a><sup>1*</sup>,</span>
              <span class="author-block">
                <a href="#">Yufan Deng</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="#">Jincheng Yu</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://lawrence-cj.github.io/">Junsong Chen</a><sup>2</sup>,</span> <br>
              <span class="author-block">
                <a href="https://www.cs.toronto.edu/~linghuan/">Huan Ling</a><sup>2</sup>,</span> 
              <span class="author-block">
                <a href="https://xieenze.github.io/">Enze Xie</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://zhoudaquan.github.io/homepage.io/">Daquan Zhou</a><sup>1</sup></span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block"><sup>1</sup>Peking University</span> 
                    <span class="author-block"><sup>2</sup>NVIDIA</span>
                    <!-- TODO: Remove this line if no equal contribution -->
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- TODO: Add your supplementary material PDF or remove this section
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Media Gallery Section -->
<section class="section media-gallery-section">
  <div class="container is-max-desktop">
    <div class="media-gallery-container">
      <div class="media-gallery">
        <!-- Video items with fixed aspect ratio -->
        <div class="media-gallery-item media-video item-v1">
          <video autoplay muted loop playsinline>
            <source src="static/videos/selected_videos/soft focus with a warm color palette, the lighting is soft and ambient, casting a gentle glow over t.mp4" type="video/mp4">
          </video>
          <div class="video-overlay">
            <i class="fas fa-expand"></i>
          </div>
        </div>
        
        <div class="media-gallery-item media-image item-i1">
          <img src="static/images/case/0_512x512.png" alt="Gallery image 1" loading="lazy">
        </div>
        
        <div class="media-gallery-item media-video item-v2">
          <video autoplay muted loop playsinline>
            <source src="static/videos/selected_videos/a picturesque view of a lush forest bathed in warm, golden sunlight. the small hedgehog, with its sh.mp4" type="video/mp4">
          </video>
          <div class="video-overlay">
            <i class="fas fa-expand"></i>
          </div>
        </div>
        
        <div class="media-gallery-item media-image item-i2">
          <img src="static/images/case/215_800x400.png" alt="Gallery image 2" loading="lazy">
        </div>
        
        <div class="media-gallery-item media-video item-v3">
          <video autoplay muted loop playsinline>
            <source src="static/videos/selected_videos/A toy robot in blue jeans and a white t-shirt leisurely walking in Antarctica as the sun sets beauti.mp4" type="video/mp4">
          </video>
          <div class="video-overlay">
            <i class="fas fa-expand"></i>
          </div>
        </div>
        
        <div class="media-gallery-item media-image item-i3">
          <img src="static/images/case/254_400x800.png" alt="Gallery image 3" loading="lazy">
        </div>
        
        <div class="media-gallery-item media-video item-v4">
          <video autoplay muted loop playsinline>
            <source src="static/videos/selected_videos/the image quality is clear and crisp, with gentle and warm lighting that highlights the secret agent.mp4" type="video/mp4">
          </video>
          <div class="video-overlay">
            <i class="fas fa-expand"></i>
          </div>
        </div>
        
        <div class="media-gallery-item media-image item-i4">
          <!-- <img src="static/images/case/70_800x400.png" alt="Gallery image 4" loading="lazy"> -->
          <video autoplay muted loop playsinline>
            <source src="static/videos/selected_videos/A lively pink pig running swiftly towards the camera in a bustling Tokyo alleyway, surrounded by neo.mp4" type="video/mp4">
          </video>
          <div class="video-overlay">
            <i class="fas fa-expand"></i>
          </div>
        </div>
        
        <div class="media-gallery-item media-video item-v5">
          <video autoplay muted loop playsinline>
            <source src="static/videos/selected_videos/3D animation of a small, round, fluffy creature with big, expressive eyes explores a vibrant, enchan.mp4" type="video/mp4">
          </video>
          <div class="video-overlay">
            <i class="fas fa-expand"></i>
          </div>
        </div>
        
        <div class="media-gallery-item media-image item-i5">
          <img src="static/images/case/12_512x512.png" alt="Gallery image 5" loading="lazy">
        </div>
        
        <div class="media-gallery-item media-video item-v6">
          <video autoplay muted loop playsinline>
            <source src="static/videos/selected_videos/Five extraterrestrial beings with sleek, metallic bodies examine a hovering, luminescent orb in a de.mp4" type="video/mp4">
          </video>
          <div class="video-overlay">
            <i class="fas fa-expand"></i>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Media Gallery Section -->

<!-- MHLA Pipeline -->
<section class="section pipeline-section">
  <div class="container is-max-desktop">
    <div class="pipeline-card">
      <img src="static/images/MHLA_overview.png" alt="MHLA pipeline overview" class="pipeline-image" loading="lazy" />
      <p class="pipeline-caption">A brief comparison bewtween vanilla linear attention and MHLA.</p>
    </div>
  </div>
</section>
<!-- End MHLA Pipeline -->

<!-- MHLA Performance Highlights -->
<section class="section performance-section">
  <div class="container is-max-desktop">
    <div class="performance-header">
      <h2 class="performance-title">A <span class="title-highlight">Universal</span> High-Efficiency Linear Attention Operator</h2>
    </div>
    
    <div class="performance-grid">
      <!-- Image Classification -->
      <div class="performance-card clickable-card" onclick="location.href='#image-classification-section'">
        <span class="card-jump-hint">Click to view details ↓</span>
        <h3 class="performance-card-title">Image Classification</h3>
        <div class="performance-metrics">
          <div class="metric-item">
            <span class="metric-label">Performance</span>
            <span class="metric-value">+3.6%</span>
          </div>
          <div class="metric-separator"></div>
          <div class="metric-item">
            <span class="metric-label">Complexity</span>
            <span class="metric-badge">Linear</span>
          </div>
        </div>
      </div>

      <!-- Image Generation -->
      <div class="performance-card clickable-card" onclick="location.href='#image-generation-section'">
        <span class="card-jump-hint">Click to view details ↓</span>
        <h3 class="performance-card-title">Image Generation</h3>
        <div class="performance-metrics">
          <div class="metric-item">
            <span class="metric-label">Performance</span>
            <span class="metric-value">+12.6%</span>
          </div>
          <div class="metric-separator"></div>
          <div class="metric-item">
            <span class="metric-label">Complexity</span>
            <span class="metric-badge">Linear</span>
          </div>
        </div>
      </div>

      <!-- Language Modeling -->
      <div class="performance-card clickable-card" onclick="location.href='#language-modeling-section'">
        <span class="card-jump-hint">Click to view details ↓</span>
        <h3 class="performance-card-title">Language Modeling</h3>
        <div class="performance-metrics">
          <div class="metric-item">
            <span class="metric-label">Performance</span>
            <span class="metric-value">+6.3%</span>
          </div>
          <div class="metric-separator"></div>
          <div class="metric-item">
            <span class="metric-label">Complexity</span>
            <span class="metric-badge">Linear</span>
          </div>
        </div>
      </div>

      <!-- Video Generation -->
      <div class="performance-card clickable-card" onclick="location.href='#video-generation-section'">
        <span class="card-jump-hint">Click to view details ↓</span>
        <h3 class="performance-card-title">Video Generation</h3>
        <div class="performance-metrics">
          <div class="metric-item">
            <span class="metric-label">Performance</span>
            <span class="metric-value">+41%</span>
          </div>
          <div class="metric-separator"></div>
          <div class="metric-item">
            <span class="metric-label">Complexity</span>
            <span class="metric-badge">Linear</span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End MHLA Performance Highlights -->



<!-- MHLA Key Features -->
<section class="section features-section">
  <div class="container is-max-desktop">
    <div class="features-header">
      <h2 class="features-title">Why MHLA?</h2>
      <p class="features-subtitle">Key Features that Set MHLA Apart</p>
    </div>

    <div class="features-grid">
      <!-- Feature 1 -->
      <div class="feature-card">
        <!-- <div class="feature-icon">
          <i class="fas fa-bolt"></i>
        </div> -->
        <h3 class="feature-title">Optimal Efficiency</h3>
        <p class="feature-description">
          For sequence lengths >1k, <br><span class="feature-highlight is-size-5">MHLA surpasses Flash Attention</span> in speed. Maintains identical efficiency with vanilla linear attention with <span class="feature-highlight">zero overhead</span>。
        </p>
      </div>

      <!-- Feature 2 -->
      <div class="feature-card">
        <!-- <div class="feature-icon">
          <i class="fas fa-arrows-alt-h"></i>
        </div> -->
        <h3 class="feature-title">Flexible Attention Forms</h3>
        <p class="feature-description">
          Natively supports both <span class="feature-highlight is-size-5">causal</span> and <span class="feature-highlight is-size-5">non-causal</span> attention modes with built-in <span class="feature-highlight">chunkwise training</span> compatibility for enhanced flexibility.
        </p>
      </div>

      <!-- Feature 3 -->
      <div class="feature-card">
        <!-- <div class="feature-icon">
          <i class="fas fa-star"></i>
        </div> -->
        <h3 class="feature-title">Token-Level Diversity</h3>
        <p class="feature-description">
          Introduces diversity at the token level to <span class="feature-highlight is-size-5">break global context collapse</span> in linear attention, unlocking significant performance gains.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End MHLA Key Features -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">About MHLA</h2>
        <div class="pipeline-card">
      <img src="static/images/MHLA_pipeline.png" alt="MHLA pipeline overview" class="pipeline-image" loading="lazy" />
      <!-- <p class="pipeline-caption">A brief comparison bewtween vanilla linear attention and MHLA.</p> -->
    </div>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution and few self-attention blocks) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a <span class="metric-highlight">3.6%</span> improvement on ImageNet classification, a <span class="metric-highlight">6.3%</span> gain on NLP, a <span class="metric-highlight">12.6%</span> improvement in image generation tasks and a <span class="metric-highlight">41%</span> enhancement in video generation tasks with the same computational complexity.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Compact Case Gallery -->
<section id="image-generation-section" class="section case-gallery-section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 performance-title has-text-centered">Image Generation</h2>
    
    <!-- MHLA vs Flash Attention - Speed & FID Chart -->
    <section class="chart-section" aria-label="MHLA vs Flash Attention speed and FID comparison">
      <div class="chart-card">
        <!-- <h3 class="title is-5 has-text-centered">Speed Comparison (Throughput)</h3> -->
        <p class="has-text-centered normal-highlight is-size-6"><span class="title-highlight">8.2× speed up</span> compared with Flash Attention</p>
        <p class="pipeline-caption"> Speed Comparison (Throughput) </p>
        <div class="bar-chart" role="img" aria-label="Bar chart showing MHLA throughput 7.40 and Flash Attention throughput 0.90">
          <div class="bar-row">
            <span class="bar-label">MHLA</span>
            <div class="bar-track">
              <div class="bar-fill mhla" style="--value: 7.40" aria-label="MHLA throughput 7.40"></div>
            </div>
            <span class="bar-value">7.40</span>
          </div>
          
          <div class="bar-row">
            <span class="bar-label">Flash Attn</span>
            <div class="bar-track">
              <div class="bar-fill flash" style="--value: 0.90" aria-label="Flash Attention throughput 0.90"></div>
            </div>
            <span class="bar-value">0.90</span>
          </div>
        </div>
        <!-- <div class="chart-divider"></div> -->
        <!-- <h3 class="title is-5 has-text-centered fid-title">Quality Comparison (FID↓)</h3> -->
        <!-- <p class="has-text-centered is-size-6">Lower is better: MHLA 59.8 vs Flash Attention 68.4</p> -->
        <div class="bar-chart fid-chart" role="img" aria-label="Bar chart showing FID scores: MHLA 59.8 and Flash Attention 68.4 (lower is better)">
          <p class="has-text-centered normal-highlight is-size-6"><span class="title-highlight">8.6 points</span> better than Flash Attention</p>
          <p class="pipeline-caption"> Quality Comparison (FID↓) </p>
          <div class="bar-row">
            <span class="bar-label">MHLA</span>
            <div class="bar-track">
              <div class="bar-fill mhla" style="--value: 30" aria-label="MHLA FID 59.8"></div>
            </div>
            <span class="bar-value">59.8</span>
          </div>
          <div class="bar-row">
            <span class="bar-label">Flash Attn</span>
            <div class="bar-track">
              <div class="bar-fill flash" style="--value: 68.4" aria-label="Flash Attention FID 68.4"></div>
            </div>
            <span class="bar-value">68.4</span>
          </div>
        </div>
      </div>
    </section>

    

    <div class="gallery-container">
      <div class="gallery" aria-label="Case image gallery">
        <div class="gallery-item item1"><img src="static/images/case/0_512x512.png" alt="Case 0 square" loading="lazy"></div>
        <div class="gallery-item item2"><img src="static/images/case/215_800x400.png" alt="Case 6 wide" loading="lazy"></div>
        <div class="gallery-item item3"><img src="static/images/case/10_400x800.png" alt="Case 10 tall" loading="lazy"></div>
        <div class="gallery-item item4"><img src="static/images/case/70_800x400.png" alt="Case 70 wide" loading="lazy"></div>
        
        <div class="gallery-item item6"><img src="static/images/case/midjourney10_800x400.png" alt="Midjourney 10 wide" loading="lazy"></div>
        <div class="gallery-item item5"><img src="static/images/case/12_512x512.png" alt="Case 12 square" loading="lazy"></div>
        <div class="gallery-item item7"><img src="static/images/case/COCOval2014000000211560_400x800.png" alt="Case 122 tall" loading="lazy"></div>
        <div class="gallery-item item8"><img src="static/images/case/254_400x800.png" alt="Case 254 tall" loading="lazy"></div>
        <div class="gallery-item item9"><img src="static/images/case/120_512x512.png" alt="Case 120 square" loading="lazy"></div>
        <div class="gallery-item item10"><img src="static/images/case/COCOval2014000000441411_800x400.png" alt="COCO val wide" loading="lazy"></div>
        <div class="gallery-item item11"><img src="static/images/case/6_800x400.png" alt="COCO val tall" loading="lazy"></div>
        <!-- <div class="gallery-item item12"><img src="static/images/case/diffusiondb17_800x400.png" alt="DiffusionDB 17 wide" loading="lazy"></div> -->
        <!-- <div class="gallery-item item13"><img src="static/images/case/partiprompts262_800x400.png" alt="Parti prompts 262 wide" loading="lazy"></div>
        <div class="gallery-item item14"><img src="static/images/case/diffusiondb0_400x800.png" alt="DiffusionDB 0 tall" loading="lazy"></div>
        <div class="gallery-item item15"><img src="static/images/case/drawtext24_512x512.png" alt="DrawText 24 square" loading="lazy"></div>
        <div class="gallery-item item16"><img src="static/images/case/diffusiondb12_512x512.png" alt="DiffusionDB 12 square" loading="lazy"></div>
        <div class="gallery-item item17"><img src="static/images/case/diffusiondb20_512x512.png" alt="DiffusionDB 20 square" loading="lazy"></div>
        <div class="gallery-item item18"><img src="static/images/case/diffusiondb5_512x512.png" alt="DiffusionDB 5 square" loading="lazy"></div>
        <div class="gallery-item item19"><img src="static/images/case/diffusiondb6_512x512.png" alt="DiffusionDB 6 square" loading="lazy"></div>
        <div class="gallery-item item20"><img src="static/images/case/partiprompts243_512x512.png" alt="Parti prompts 243 square" loading="lazy"></div>
        <div class="gallery-item item21"><img src="static/images/case/partiprompts27_400x800.png" alt="Parti prompts 27 tall" loading="lazy"></div>
        <div class="gallery-item item22"><img src="static/images/case/partiprompts48_512x512.png" alt="Parti prompts 48 square" loading="lazy"></div>
        <div class="gallery-item item23"><img src="static/images/case/partiprompts50_512x512.png" alt="Parti prompts 50 square" loading="lazy"></div>
        <div class="gallery-item item24"><img src="static/images/case/COCOval2014000000425925_512x512.png" alt="COCO val square" loading="lazy"></div> -->
      </div>
    </div>
    <p class="subtitle is-6 gallery-subtitle has-text-centered">Images generated with SANA-MHLA</p>
  </div>
</section>



<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
  
      <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata">
  
        <source src="static/videos/banner_video.mp4" type="video/mp4">
      </video>

      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section>  -->
<!-- End teaser video -->


<!-- Image carousel (commented out)
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/carousel1.jpg" alt="First research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel2.jpg" alt="Second research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel3.jpg" alt="Third research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      <img src="static/images/carousel4.jpg" alt="Fourth research result visualization" loading="lazy"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
-->
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">

      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">

            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<section id="video-generation-section" class="hero is-small">

    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Video Generation</h2>
      
      <!-- VBench Score & Latency Charts -->
      <section class="chart-section video-charts" aria-label="Video generation performance comparison">
        <div class="chart-card">
          <!-- VBench Score Chart -->
          <div class="video-chart-half">
            <p class="has-text-centered normal-highlight is-size-6">
              <span class="title-highlight">MHLA</span> achieves the same VBench score as Flash Attention
            </p>
            <p class="pipeline-caption">VBench Score Comparison</p>
            <div class="bar-chart video-bar-chart" role="img" aria-label="VBench Score comparison chart">
              <div class="bar-row">
                <span class="bar-label">Wan2.1 1.3B</span>
                <div class="bar-track">
                  <div class="bar-fill flash" style="--value: 83.31" aria-label="Wan2.1 1.3B VBench Score 83.31"></div>
                </div>
                <span class="bar-value">83.31</span>
              </div>
              <div class="bar-row">
                <span class="bar-label">Full Linear</span>
                <div class="bar-track">
                  <div class="bar-fill flash" style="--value: 58.24" aria-label="Full Linear VBench Score 58.24"></div>
                </div>
                <span class="bar-value">58.24</span>
              </div>
              <div class="bar-row">
                <span class="bar-label">Full MHLA</span>
                <div class="bar-track">
                  <div class="bar-fill mhla" style="--value: 82.83" aria-label="Full MHLA VBench Score 82.83"></div>
                </div>
                <span class="bar-value">82.83</span>
              </div>
              <div class="bar-row">
                <span class="bar-label">Hybrid 2/3 MHLA</span>
                <div class="bar-track">
                  <div class="bar-fill mhla" style="--value: 83.82" aria-label="Hybrid 2/3 MHLA VBench Score 83.82"></div>
                </div>
                <span class="bar-value">83.82</span>
              </div>
            </div>
          </div>

          <!-- Latency Chart -->
          <div class="video-chart-half">
            <p class="has-text-centered normal-highlight is-size-6">
              <span class="title-highlight">2.2× faster</span> than Flash Attention
            </p>
            <p class="pipeline-caption">Latency Comparison (Lower is Better)</p>
            <div class="bar-chart video-bar-chart latency-chart" role="img" aria-label="Latency comparison chart">
              <div class="bar-row">
                <span class="bar-label">Wan2.1 1.3B</span>
                <div class="bar-track">
                  <div class="bar-fill flash" style="--value: 139" aria-label="Wan2.1 1.3B Latency 139s"></div>
                </div>
                <span class="bar-value">139s</span>
              </div>
              <div class="bar-row">
                <span class="bar-label">Full Linear</span>
                <div class="bar-track">
                  <div class="bar-fill flash" style="--value: 62" aria-label="Full Linear Latency 62s"></div>
                </div>
                <span class="bar-value">62s</span>
              </div>
              <div class="bar-row">
                <span class="bar-label">Full MHLA</span>
                <div class="bar-track">
                  <div class="bar-fill mhla" style="--value: 62" aria-label="Full MHLA Latency 62s"></div>
                </div>
                <span class="bar-value">62s</span>
              </div>
              <div class="bar-row">
                <span class="bar-label">Hybrid 2/3 MHLA</span>
                <div class="bar-track">
                  <div class="bar-fill mhla" style="--value: 84" aria-label="Hybrid 2/3 MHLA Latency 84s"></div>
                </div>
                <span class="bar-value">84s</span>
              </div>
            </div>
          </div>
        </div>

      </section>

      <div class="media-gallery-container">
        <div class="media-gallery video-gallery-2rows">

          <div class="media-gallery-item media-video video-gallery-item">
            <video autoplay muted loop playsinline>
              <source src="static/videos/selected_videos/A curious tabby cat leans out of a window, intently observing the world outside with sparkling green.mp4" type="video/mp4">
            </video>
            <div class="video-overlay">
              <i class="fas fa-expand"></i>
            </div>
          </div>
          
          <div class="media-gallery-item media-video video-gallery-item">
            <video autoplay muted loop playsinline>
              <source src="static/videos/selected_videos/Extreme close up of a 24 year old woman’s eye blinking, standing in Marrakech during magic hour, cin.mp4" type="video/mp4">
            </video>
            <div class="video-overlay">
              <i class="fas fa-expand"></i>
            </div>
          </div>
          
          <div class="media-gallery-item media-video video-gallery-item">
            <video autoplay muted loop playsinline>
              <source src="static/videos/selected_videos/Panda playing the guitar.mp4" type="video/mp4">
            </video>
            <div class="video-overlay">
              <i class="fas fa-expand"></i>
            </div>
          </div>
          
          <!-- <div class="media-gallery-item media-video video-gallery-item">
            <video autoplay muted loop playsinline>
              <source src="static/videos/selected_videos/A tranquil river flows gently beneath a rustic wooden bridge, with soft ripples and reflections of t.mp4" type="video/mp4">
            </video>
            <div class="video-overlay">
              <i class="fas fa-expand"></i>
            </div>
          </div> -->
          
          <div class="media-gallery-item media-video video-gallery-item">
            <video autoplay muted loop playsinline>
              <source src="static/videos/selected_videos/A tropical fish swimming in ocean reefs.mp4" type="video/mp4">
            </video>
            <div class="video-overlay">
              <i class="fas fa-expand"></i>
            </div>
          </div>
          
          <div class="media-gallery-item media-video video-gallery-item">
            <video autoplay muted loop playsinline>
              <source src="static/videos/selected_videos/Animated scene features a close-up of a short fluffy monster kneeling beside a melting red candle. T.mp4" type="video/mp4">
            </video>
            <div class="video-overlay">
              <i class="fas fa-expand"></i>
            </div>
          </div>
          
          <div class="media-gallery-item media-video video-gallery-item">
            <video autoplay muted loop playsinline>
              <source src="static/videos/selected_videos/Macro shot to the face freckles of a young woman trying to look for something..mp4" type="video/mp4">
            </video>
            <div class="video-overlay">
              <i class="fas fa-expand"></i>
            </div>
          </div>
          
          <!-- Row 3: 3 videos -->
          <div class="media-gallery-item media-video video-gallery-item">
            <video autoplay muted loop playsinline>
              <source src="static/videos/selected_videos/A woman surrounded by swirling smoke of vibrant colors, warm light bathing her figure. Medium shot, .mp4" type="video/mp4">
            </video>
            <div class="video-overlay">
              <i class="fas fa-expand"></i>
            </div>
          </div>
          
          <div class="media-gallery-item media-video video-gallery-item">
            <video autoplay muted loop playsinline>
              <source src="static/videos/selected_videos/3D animation of a small, round, fluffy creature with big, expressive eyes explores a vibrant, enchan.mp4" type="video/mp4">
            </video>
            <div class="video-overlay">
              <i class="fas fa-expand"></i>
            </div>
          </div>
          
          <div class="media-gallery-item media-video video-gallery-item">
            <video autoplay muted loop playsinline>
              <source src="static/videos/selected_videos/aerial shot using a drone, the image quality is high with soft morning light creating a gentle atmos.mp4" type="video/mp4">
            </video>
            <div class="video-overlay">
              <i class="fas fa-expand"></i>
            </div>
          </div>
        </div>
      </div>
    </div>

</section>
<!-- End video carousel -->


<!-- Image Classification -->
<section id="image-classification-section" class="section case-gallery-section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 performance-title has-text-centered">Image Classification</h2>
    
    <section class="chart-section" aria-label="Image classification accuracy comparison at different resolutions">
      <div class="chart-card">
        <p class="has-text-centered normal-highlight is-size-6">
          <span class="title-highlight">MHLA outperforms</span> Self Attn across multiple resolutions
        </p>
        <p class="pipeline-caption">Accuracy Comparison on DeiT-T (ACC %)</p>
        
        <div class="classification-charts">
          <!-- 224x224 Resolution -->
          <div class="classification-chart-group">
            <p class="has-text-centered is-size-6" style="font-weight: 600; margin-bottom: 0.5rem;">224×224</p>
            <div class="bar-chart classification-bar-chart" role="img" aria-label="Accuracy comparison at 224x224 resolution">
              <div class="bar-row-slim">
                <span class="bar-label-slim">SA</span>
                <div class="bar-track">
                  <div class="bar-fill flash" style="--value: 22.2" aria-label="SA 224x224 ACC 72.2"></div>
                </div>
                <span class="bar-value">72.2</span>
              </div>
              <div class="bar-row-slim">
                <span class="bar-label-slim">MHLA</span>
                <div class="bar-track">
                  <div class="bar-fill mhla" style="--value: 25.8" aria-label="MHLA 224x224 ACC 75.8"></div>
                </div>
                <span class="bar-value">75.8</span>
              </div>
            </div>
          </div>

          <!-- 384x384 Resolution -->
          <div class="classification-chart-group">
            <p class="has-text-centered is-size-6" style="font-weight: 600; margin-bottom: 0.5rem;">384×384</p>
            <div class="bar-chart classification-bar-chart" role="img" aria-label="Accuracy comparison at 384x384 resolution">
              <div class="bar-row-slim">
                <span class="bar-label">SA</span>
                <div class="bar-track">
                  <div class="bar-fill flash" style="--value: 24.4" aria-label="SA 384x384 ACC 74.4"></div>
                </div>
                <span class="bar-value">74.4</span>
              </div>
              <div class="bar-row-slim">
                <span class="bar-label">MHLA</span>
                <div class="bar-track">
                  <div class="bar-fill mhla" style="--value: 27.5" aria-label="MHLA 384x384 ACC 77.5"></div>
                </div>
                <span class="bar-value">77.5</span>
              </div>
            </div>
          </div>

          <!-- 512x512 Resolution -->
          <div class="classification-chart-group">
            <p class="has-text-centered is-size-6" style="font-weight: 600; margin-bottom: 0.5rem;">512×512</p>
            <div class="bar-chart classification-bar-chart" role="img" aria-label="Accuracy comparison at 512x512 resolution">
              <div class="bar-row">
                <span class="bar-label">SA</span>
                <div class="bar-track">
                  <div class="bar-fill flash" style="--value: 25.3" aria-label="SA 512x512 ACC 75.3"></div>
                </div>
                <span class="bar-value">75.3</span>
              </div>
              <div class="bar-row">
                <span class="bar-label">MHLA</span>
                <div class="bar-track">
                  <div class="bar-fill mhla" style="--value: 28.3" aria-label="MHLA 512x512 ACC 78.3"></div>
                </div>
                <span class="bar-value">78.3</span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
  </div>
</section>
<!-- End Image Classification -->


<!-- Language Modeling -->
<section id="language-modeling-section" class="section case-gallery-section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 performance-title has-text-centered">Language Modeling</h2>
    
    <section class="chart-section" aria-label="Language modeling performance comparison">
      <div class="chart-card">
        <p class="has-text-centered normal-highlight is-size-6">
          <span class="title-highlight">MHLA leads</span> in both CSR and LongBench benchmarks
        </p>
        
        <div class="language-modeling-charts">
          <!-- CSR avg Chart -->
          <div class="lm-chart-half">
            <p class="pipeline-caption">CSR Average</p>
            <div class="bar-chart lm-bar-chart" role="img" aria-label="CSR average comparison">
              <div class="bar-row">
                <span class="bar-label">GLA</span>
                <div class="bar-track">
                  <div class="bar-fill flash" style="--value: 3.0" aria-label="GLA CSR avg 46.0"></div>
                </div>
                <span class="bar-value">46.0</span>
              </div>
              <div class="bar-row">
                <span class="bar-label">Transformer++</span>
                <div class="bar-track">
                  <div class="bar-fill flash" style="--value: 3.8" aria-label="Transformer++ CSR avg 46.8"></div>
                </div>
                <span class="bar-value">46.8</span>
              </div>
              <div class="bar-row">
                <span class="bar-label">Mamba</span>
                <div class="bar-track">
                  <div class="bar-fill flash" style="--value: 3.4" aria-label="Mamba CSR avg 46.4"></div>
                </div>
                <span class="bar-value">46.4</span>
              </div>
              <div class="bar-row">
                <span class="bar-label">Mamba2</span>
                <div class="bar-track">
                  <div class="bar-fill flash" style="--value: 4.0" aria-label="Mamba2 CSR avg 47.0"></div>
                </div>
                <span class="bar-value">47.0</span>
              </div>
              <div class="bar-row">
                <span class="bar-label">GDN</span>
                <div class="bar-track">
                  <div class="bar-fill flash" style="--value: 3.9" aria-label="GDN CSR avg 46.9"></div>
                </div>
                <span class="bar-value">46.9</span>
              </div>
              <div class="bar-row">
                <span class="bar-label">MHLA</span>
                <div class="bar-track">
                  <div class="bar-fill mhla" style="--value: 4.1" aria-label="MHLA CSR avg 47.1"></div>
                </div>
                <span class="bar-value">47.1</span>
              </div>
            </div>
          </div>

          <!-- LongBench avg Chart -->
          <div class="lm-chart-half">
            <p class="pipeline-caption">LongBench Average</p>
            <div class="bar-chart lm-bar-chart longbench-chart" role="img" aria-label="LongBench average comparison">
              <div class="bar-row">
                <span class="bar-label">GLA</span>
                <div class="bar-track">
                  <div class="bar-fill flash" style="--value: 1.53" aria-label="GLA LongBench avg 6.53"></div>
                </div>
                <span class="bar-value">6.53</span>
              </div>
              <div class="bar-row">
                <span class="bar-label">Transformer++</span>
                <div class="bar-track">
                  <div class="bar-fill flash" style="--value: 1.92" aria-label="Transformer++ LongBench avg 6.92"></div>
                </div>
                <span class="bar-value">6.92</span>
              </div>
              <div class="bar-row">
                <span class="bar-label">Mamba</span>
                <div class="bar-track">
                  <div class="bar-fill flash" style="--value: 1.97" aria-label="Mamba LongBench avg 6.97"></div>
                </div>
                <span class="bar-value">6.97</span>
              </div>
              <div class="bar-row">
                <span class="bar-label">Mamba2</span>
                <div class="bar-track">
                  <div class="bar-fill flash" style="--value: 1.62" aria-label="Mamba2 LongBench avg 6.62"></div>
                </div>
                <span class="bar-value">6.62</span>
              </div>
              <div class="bar-row">
                <span class="bar-label">GDN</span>
                <div class="bar-track">
                  <div class="bar-fill flash" style="--value: 1.86" aria-label="GDN LongBench avg 6.86"></div>
                </div>
                <span class="bar-value">6.86</span>
              </div>
              <div class="bar-row">
                <span class="bar-label">MHLA</span>
                <div class="bar-track">
                  <div class="bar-fill mhla" style="--value: 2.41" aria-label="MHLA LongBench avg 7.41"></div>
                </div>
                <span class="bar-value">7.41</span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
  </div>
</section>
<!-- End Language Modeling -->


<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Paper</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{MHLA2025,
  title={MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head},
  author={Zhang, Kewei and Huang, Ye and Deng, Yufan and Yu, Jincheng and Chen, Junsong and Ling, Huan and Xie, Enze and Zhou, Daquan},
  journal={Conference/Journal Name},
  year={2025},
  url={https://your-domain.com/your-project-page}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            <!-- This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page. -->
            This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            <br>
            Magic Group | Peking University
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
